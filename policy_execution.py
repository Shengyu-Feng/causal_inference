import torch
import torch.nn.functional as F
import numpy as np
import heapq
from sklearn.linear_model import Ridge
import matplotlib.pyplot as plt
from betaVAE import VAE
from a2c_ppo_acktr.envs import make_vec_envs
from torch.distributions.bernoulli import Bernoulli
from torch.utils.data import Dataset, DataLoader, random_split
from torch.nn import CrossEntropyLoss
from torch import nn, optim

mix_state_size = 50000 # size of states generated by mix policy
sample_size = 10000 # sample size to calculate the average KL divergence
interest_size = 10000 # size for target intervention
latent_size = 30

def sample(weights, temperature):
    return Bernoulli(logits=torch.from_numpy(weights) / temperature).sample().long().numpy()

def linear_regression(masks, rewards, alpha=1.0):
    model = Ridge(alpha).fit(masks, rewards)
    return model.coef_, model.intercept_

def visualize_states(model, obs):
    # model: VAE
    # obs of shape [N, 4,84,84]
    if obs.ndim == 3:
        obs = obs.unsqueeze(0)
    batch_size = obs.size(0)
    obs = obs.cuda()
    model.cuda()
    with torch.no_grad():
        recon_img, _, _ = model(obs)
    fig=plt.figure(figsize=(6*(batch_size+1), 20))
    for i in range(batch_size):
        fig.add_subplot(2*batch_size,1,2*i+1)
        plt.imshow(obs[i].permute(0,2,1).cpu().view(84*4,84).permute(1,0))
        fig.add_subplot(2*batch_size,1,2*i+2)
        plt.imshow(recon_img[i].permute(0,2,1).cpu().view(84*4,84).permute(1,0))
    plt.show()

class MLP(nn.Module):
    """Helper module to create MLPs."""
    def __init__(self, dims, activation=nn.ReLU):
        super().__init__()
        blocks = nn.ModuleList()

        for i, (dim_in, dim_out) in enumerate(zip(dims, dims[1:])):
            blocks.append(nn.Linear(dim_in, dim_out))

            if i < len(dims)-2:
                blocks.append(activation())

        self.blocks = nn.Sequential(*blocks)

    def forward(self, x):
        return self.blocks(x)
    

class Imitator(nn.Module):
    def __init__(self,encoder, net):
        super(Imitator, self).__init__()
        self.encoder = encoder
        self.net = net;
        
    def forward(self, X, G):
        # G (b, s, d)
        # s is sample size
        with torch.no_grad():
            self.z = self.encoder(X)[:,:latent_size]
        self.z = self.z.unsqueeze(1)
        self.z = self.z.repeat((1,G.size(1),1))
        action = self.net(torch.cat([self.z*G,G],axis=2))
        return action

disentangle_model = VAE(latent_size)
states = torch.load("checkpoints/checkpoint-30D-130")
disentangle_model.load_state_dict(states["model_states"])

model = Imitator(disentangle_model.encoder, MLP([60, 500, 500, 300, 6]))
model.load_state_dict(torch.load("models/graph_parameterized.pt"))
model.cuda()
env = make_vec_envs(
    "PongNoFrameskip-v4",
    1000,
    1,
    None,
    None,
    device='cuda',
    allow_early_resets=False)
    
def policy_execution(model, env, G):
    obs = env.reset()
    rewards = 0
    done = False
    while not done:
        with torch.no_grad():
            action = model(obs,G)
            action = action.squeeze(1)
        obs, reward, done, _ = env.step(torch.argmax(action,1))
        rewards += reward
    return rewards

G_list = []
rg_list = []
temperature = 1
w = np.zeros(30)

for i in range(2000):
    G = sample(w,temperature)
    G_list.append(G)
    rg = np.mean([policy_execution(model,env,torch.Tensor(G).view(1,1,-1).cuda()).item() for i in range(2)])
    rg_list.append(rg)
    print(rg.item())
    w, _ = linear_regression(G_list, rg_list, alpha=1.0)